<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Payton</title>
  <link rel="stylesheet" href="style.css"/>
  <script src="./sectionscrip.js" defer></script>
</head>
<body>
  <div class="linkback">
    <div class="flex-container">
      <p class="mainhead">Learn More:</p>
      <div class="links">
        <p><a href="https://github.com/Paytonmen" target="_blank" rel="noopener noreferrer">Github</a></p>
        <p><a href="https://www.linkedin.com/in/payton-mensch" target="_blank" rel="noopener noreferrer">LinkedIn</a></p>
        <p>Games (in progress)</p>
      </div>
    </div>
  </div>

  <div class="nameback">
    <p class="name">Payton Mensch</p>
    <p class="roles">Veteran | AI/ML Engineer | Software Dev</p>
  </div>

  <div class="pagesback">
    <div class="navbar" role="navigation" aria-label="Primary">
      <p class="nav-link" onclick="showSection('resume')">Resume</p>
      <p class="nav-link" onclick="showSection('about')">About</p>
      <p class="nav-link" onclick="showSection('awards')">Awards</p>
      <p class="nav-link"onclick="showSection('contact')">Contact</p>

      <div class="nav-item">
        <p class="nav-link" tabindex="0" aria-haspopup="true" aria-expanded="false">Projects â–¾</p>
        <div class="dropdown-menu" role="menu" aria-label="Projects submenu">
          <a href="#" role="menuitem" onclick="showSection('ml-translation'); return false;">ML Translation</a>
          <a href="#" role="menuitem" onclick="showSection('predictive-logistics'); return false;">Predictive Logistics</a>
        </div>
      </div>
    </div>
  </div>
  <div id="resume" class="contentsecs hidden">
    <div class="resflex-container">
      <img src="./images/linkedin.jpeg" alt="Paytons profile picutre">
      <div class="resume-text">
        <p>U.S. Army Veteran, experienced in Front/Back-end development, AI/ML Ops centered around NLP, Machine Translation, and Logistics Analysis.</p>
        <p>Please click <a href='./files/Payton-Resume-Sep.docx' download="Payton-Resume-Sep.docx"><em><strong>Here</strong></em></a> to download a docx Version of my resume, and 
        <a href="./files/Payton-Resume-Sep.pdf" download="Payton-Resume-Sep.pdf"><em><strong>Here</strong></em> for a PDF version!</a></p>
      </div>
    </div>
  </div>
  <div id="welcome" class="contentsecs welcome">
    <h2>Thanks For Visitng!</h2>
    <p>Please click one of the links to learn more about me, or my projects, and to connect on social media.</p>
  </div>
  <div id="about" class="contentsecs hidden">
    <div class="resume-text">
      <h2>About Me</h2>
      <p>I enlisted in the Army as a radio communication technician in Nov. of 2016. After finishing my basic and job training, I went to my first duty station
        at Fort Wainwright, AK. In Oct. of 2020, I transferred to my second duty station of Fort Carson, CO, which I remained until the present year.</p>
      <p>While my first 8 years of service was primarily focused around hardware repair, I have transitioned to working in the software and AI/ML role since May of 2024.
        During this time, I have created several AI/Ml products, such as ML-powered offline translation tools in Thai and Mandarin to support units working in the Pacific,
        as well as predictive-logistics products such as time-series forecasting for demand analysis.</p>
      <p>Currently, I am looking to return home to North Carolina as I retire from the military and pursue new opportunities!</p>
    </div>
  </div>
  <div id="awards" class="contentsecs hidden">
    <div class="resume-text">
      <h2>Awards and Recognition</h2>
      <p>Abstract on AI-Translation published by West Point Press 
        <a href="https://static1.squarespace.com/static/6239ec5f3f7611307897a1f0/t/686d305014c2f71562591ce8/1751986262119/The+Fourth+Tactical+Innovation+Symposium+Technical+and+Procedural+Abstracts.pdf" target="_blank" rel="noopener noreferrer"><em><strong>Here</strong></em></a>
      </p>
      <h3>Certificates</h3>
      <p>Nvidia Fundamentals of Deep Learning certificate</p>
      <p>Nvidia Generative AI and Diffusion Models certificate</p>
      <p>AC/DC Circuits</p>
      <h3>Awards</h3>
      <p>Army Commendation Medal, 2</p>
      <p>Army Achievement Medal, 5</p>
      <p>Army Good Conduct Medal, 2</p>
    </div>
  </div>
  <div id="contact" class="contentsecs hidden">
    <div class="resume-text">
      <h2>Contact me at:</h2>
      <p>paytonmensch@yahoo.com</p>
      <p>984-222-5921</p>
      <p>LinkedIn at the link on the banner</p>
      <p>Through git using the link on the banner</p>
    </div>
  </div>
  <div id="ml-translation" class="contentsecs hidden">
    <h2>Machine Learning Translation</h2>
    <h3>How it started</h3>
    <p>When I first began this project, it arose from the desire to provide local, offline translation capabilities for units that may not have
      enough vetted translators to support their missions. From there, it grew to providing a fully-functional tool developed using open-source resources
      and datasets to provide rapid-deployable translation capabilities on low-resource devices.
    </p>
    <h3>How it was made</h3>
    <p>
      The first version of the translation tool was targeting English - Thai, and Thai - English. I quickly ran into the issue of finding an open-source
      dataset to train from that wasn't asking for a payment. To resolve this issue, I implemented a strategy where I combed through online fan-made movie
      and TV show sub-title translations. For the second version, focusing on Mandarin, I was able to find a UN corpus of paired English and Mandarin speeches
      and documents.
    </p>
    <h3>The Datasets</h3>
    <p>
      One of the challenges in this project was the datasets. While the UN set was fairly well-designed, the custom-made sub-titles dataset required some refinement.
      Although movie and TV show subs are already a good dataset in theory, as they are temporally co-paired by design, there were some issues with variations in
      the specific sentences and times. For example, a few sentences were split on different lines, or were off by a very small margin in the timer. To fix this,
      I simply ran a pre-process script that looked for punctuation marks to determine if a line was randomly broke apart, and then verified the few edge cases that
      were just weirdly formatted. The Thai-English corpus ended up at around 300,000 words, and the Mandarin-English corpus was a little less. I acknowledge
      that these are considered very lightweight sizes.
    </p>
    <p>
      For the Mandarin model, the UN provided excellent paired datasets, which can be utilized essentially out of the box after downloading, with very minor tweaks
      to suit it for my specific training.
    </p>
    <h3>Libraries and Tools</h3>
    <p>
      As impressive as reinventing the wheel on my laptop would be, there was no reason to do so here. Consequently, I utilized several open-source tools
      and common python Libraries to get this project up.
    </p>
    <p>
      Libraries used include transformers, torch, pandas, glob (for multiple .tsv files), datasets, and finally pyinstaller to package it as an executable app.
      Additionally, I used a few open-source tools like huggingface from Facebook and the mbart50fast tokenizer. For voice recognition, I used Vosk, as I don't have
      the resources or time to train a voice model.
    </p>
    <h3>Model Training</h3>
    <p>
      The actual training comprised of using tokenization to break apart sentence structures from my paired-corpus, then training with transformer's trainer.
      If I was looking to create a much stronger generative model with stronger resources for training, I would have used seq2seqtrainer instead of trainer.
      That said, each direction for training took approximately 170 hours of run-time, during which my laptop was struggling to run anything else, so resource
      limitations heavily influenced my selections. However, the end goal of this was to run on low-resource devices, so training for generative implementation
      would probably have yielded an unusable product anyways.
    </p>
    <h3>Results and Improvements</h3>
    <p>
      The trained models demonstrated a robust capability to provide relatively quick translation of shorter sentences. When asking sentences of around 15-20
      words or less, the time to translate on a low-resource computer was about 20s, while it was below 5s on my own laptop using a stronger CPU. With CUDA enabled
      devices, short sentence translation was under 2s. Longer sentences begin to scale longer translation times, but are still within reasonable waits. Additionally,
      the models preformed well on finding alternate words that fit the general idea for complex ideas, if not providing 100% accurate context. An example of this
      was that "jungle" was translated to "forest" as the word had been missed in the Thai corpus. This level of accuracy is acceptable for general usage, but
      if used in high-fidelity requirements, it would need refinement.
    </p>
    <p>
      If I had more time and resources to devote solely to this project, I would like to focus on three major improvements. Increase the corpus size into millions
      of paired words to capture more breadth in recognition, curate the dataset more to ensure accurate translation, and implement seq2seqtrainer during the
      training step to prepare for generative capabilities.
    </p>
  </div>
  <div id="predictive-logistics" class="contentsecs hidden">
    <h2>Predictive Logistics</h2>
    <p>This is a little shorter due to controlled information being used in the project. The question was essentially if ML tools could empower
      military decision making by providing better estimates of logistics data such as part-demands and operational status of fleets.
    </p>
    <h3>Model Design</h3>
    <p>Several iterations were made testing different options such as Prophet, SARIMA, CNN, etc. What I eventually settled on was an LSTM ensemble model. 
      The LSTM had better results I believe due to customizing the memory weights and lags. What I ended up doing was running two LSTM models, one to analyze for
      group trends, which can be conceptualized as replacing brakes and tires at the same time, and then another LSTM to analyze each part individually. Since there 
      are a lot of parts that are legacy parts, I ended up limiting this to the top 20 frequently ordered parts. Finally, the model takes a NIIN (material ID number) + date payload and returns
      the next 12 months of predicted quantities on a monthly basis.
    </p>
    <h3>Results and Improvements</h3>
    <p>
      This model when evaluating using MASE, MSE, etc. Demonstrated better results 2/3rds of the time when compared to traditional averaging methods being used. 
      Ideally, I would like to improve the modeling with better curation of the parts, and finding an exogenous variable to help predict as opposed to purely a
      seasonality-based approach using ordering history. Additionally, the model would benefit greatly from more accurate reporting of part demand/ordering, as
      there are some gaps and suspicious data.
    </p>
  </div>
</body>
</html>
